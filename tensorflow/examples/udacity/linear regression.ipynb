{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def import_from_csv(path, pixel_depth):\n",
    "    train_database = np.genfromtxt('{}'.format(path), delimiter=\",\", dtype=int)[1:,:]\n",
    "    train_labels = train_database[:,0].reshape(42000,)\n",
    "    training_labels = np.eye(10)[train_labels]\n",
    "    training_data = np.delete(train_database,0,1)\n",
    "    return normalize(training_data, pixel_depth), training_labels, train_labels\n",
    "\n",
    "def normalize(image_data, pixel_depth):\n",
    "    data = (image_data - pixel_depth / 2) / pixel_depth\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#training_data = np.random.randn(20000, 784)\n",
    "\n",
    "#training_weights = np.random.randn(10, 784)\n",
    "#training_response = np.dot(training_data, training_weights.T)\n",
    "\n",
    "#correct_index = np.random.choice(10, np.shape(training_data)[0])\n",
    "#training_response = np.eye(10)[correct_index]\n",
    "\n",
    "training_data, training_response, _ = import_from_csv('/Users/JAustin/Desktop/MNIST/train.csv', 255)\n",
    "\n",
    "data_size = np.shape(training_data)[0] # 20000\n",
    "num_params = np.shape(training_data)[1] # 200\n",
    "num_classes = np.shape(training_response)[1] # 10\n",
    "\n",
    "weights = np.random.randn(num_classes, num_params) # or randn\n",
    "biases = np.random.randn(num_classes, 1)\n",
    "\n",
    "#learning_rate = .1 * np.ones(iterations) # constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "iterations = 400\n",
    "\n",
    "# learning_rate = .005*np.exp(-5*np.arange(0,iterations)/iterations) # exponential learning rate\n",
    "# learning_rate = np.array([.05*(1 - x/iterations) for x in range(iterations)]) # linear learning rate\n",
    "\n",
    "\n",
    "def predict(data, batch_size, weights, biases):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    prediction = np.dot(data[index], weights.T) + biases.T\n",
    "    return index, prediction\n",
    "\n",
    "def least_square_optimization(data, response, weights, biases, batch_size, iterations, learning_rate):\n",
    "    for i in range(iterations):\n",
    "        index, prediction = predict(data, batch_size, weights, biases)\n",
    "        error_arr = response[index] - prediction\n",
    "        loss = np.tensordot(error_arr, error_arr) / (2*batch_size)\n",
    "        print(\"Loss at step %s is %s\" % (i, loss))\n",
    "        dW = np.zeros_like(weights)\n",
    "        dB = np.zeros_like(biases)\n",
    "        for j in range(np.shape(weights)[0]):\n",
    "            for k in range(np.shape(weights)[1]):\n",
    "                dW[j,k]= - np.sum((response[index]-prediction)[:, j] * data[index][:, k]) / batch_size\n",
    "        for j in range(len(biases)):\n",
    "            dB[j]= - np.sum((response[index]-prediction)[:, j]) / batch_size\n",
    "\n",
    "        weights = weights - learning_rate[i]*dW\n",
    "        biases = biases - learning_rate[i]*dB\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "def evaluate(data, labels, weights, biases, batch_size):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    accuracy = (np.argmax(np.dot(data[index], weights.T) + biases.T, axis=1) == np.argmax(training_response[index], axis=1)).sum() * 100 / batch_size\n",
    "    print(\"The accuracy of your model is %s%%!\" % accuracy)\n",
    "    return accuracy, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 795.232690104\n",
      "Loss at step 1 is 369.590283871\n",
      "Loss at step 2 is 315.500165297\n",
      "Loss at step 3 is 244.897484557\n",
      "Loss at step 4 is 266.580972277\n",
      "Loss at step 5 is 222.600351867\n",
      "Loss at step 6 is 230.246881797\n",
      "Loss at step 7 is 200.248824676\n",
      "Loss at step 8 is 272.850452311\n",
      "Loss at step 9 is 221.820196572\n",
      "Loss at step 10 is 202.023717393\n",
      "Loss at step 11 is 196.336955429\n",
      "Loss at step 12 is 223.617156578\n",
      "Loss at step 13 is 200.736334036\n",
      "Loss at step 14 is 220.798736695\n",
      "Loss at step 15 is 207.211813421\n",
      "Loss at step 16 is 199.203246118\n",
      "Loss at step 17 is 196.612041976\n",
      "Loss at step 18 is 193.850696833\n",
      "Loss at step 19 is 163.962296896\n",
      "Loss at step 20 is 175.101777475\n",
      "Loss at step 21 is 146.810590805\n",
      "Loss at step 22 is 167.125801073\n",
      "Loss at step 23 is 121.059693078\n",
      "Loss at step 24 is 182.25534537\n",
      "Loss at step 25 is 165.228282042\n",
      "Loss at step 26 is 175.598057212\n",
      "Loss at step 27 is 152.473524408\n",
      "Loss at step 28 is 140.334536051\n",
      "Loss at step 29 is 174.103117244\n",
      "Loss at step 30 is 170.380712696\n",
      "Loss at step 31 is 128.742751562\n",
      "Loss at step 32 is 130.931231952\n",
      "Loss at step 33 is 143.631380848\n",
      "Loss at step 34 is 141.09939798\n",
      "Loss at step 35 is 166.563518224\n",
      "Loss at step 36 is 137.120297116\n",
      "Loss at step 37 is 143.317678249\n",
      "Loss at step 38 is 143.398900589\n",
      "Loss at step 39 is 155.347517111\n",
      "Loss at step 40 is 153.76419948\n",
      "Loss at step 41 is 117.411614517\n",
      "Loss at step 42 is 132.498688939\n",
      "Loss at step 43 is 159.462621252\n",
      "Loss at step 44 is 153.735747595\n",
      "Loss at step 45 is 106.508483622\n",
      "Loss at step 46 is 121.398614105\n",
      "Loss at step 47 is 143.280406474\n",
      "Loss at step 48 is 109.166082814\n",
      "Loss at step 49 is 109.135552938\n",
      "Loss at step 50 is 134.695401951\n",
      "Loss at step 51 is 121.534314894\n",
      "Loss at step 52 is 143.409548086\n",
      "Loss at step 53 is 114.272772693\n",
      "Loss at step 54 is 105.64659623\n",
      "Loss at step 55 is 133.501249857\n",
      "Loss at step 56 is 125.091111161\n",
      "Loss at step 57 is 138.619867516\n",
      "Loss at step 58 is 148.078909959\n",
      "Loss at step 59 is 124.310764318\n",
      "Loss at step 60 is 113.172524963\n",
      "Loss at step 61 is 124.293955609\n",
      "Loss at step 62 is 83.650759683\n",
      "Loss at step 63 is 111.865056194\n",
      "Loss at step 64 is 90.9084340481\n",
      "Loss at step 65 is 85.2670089677\n",
      "Loss at step 66 is 88.8947548864\n",
      "Loss at step 67 is 114.204599833\n",
      "Loss at step 68 is 101.658404492\n",
      "Loss at step 69 is 100.311238372\n",
      "Loss at step 70 is 105.140469815\n",
      "Loss at step 71 is 77.9255348834\n",
      "Loss at step 72 is 106.48546153\n",
      "Loss at step 73 is 105.515081312\n",
      "Loss at step 74 is 105.114480508\n",
      "Loss at step 75 is 138.579220708\n",
      "Loss at step 76 is 117.622111365\n",
      "Loss at step 77 is 129.412564251\n",
      "Loss at step 78 is 96.4663747494\n",
      "Loss at step 79 is 85.3348235967\n",
      "Loss at step 80 is 103.238202492\n",
      "Loss at step 81 is 92.8796009751\n",
      "Loss at step 82 is 89.7143946494\n",
      "Loss at step 83 is 78.5797652598\n",
      "Loss at step 84 is 113.001999347\n",
      "Loss at step 85 is 87.7868053885\n",
      "Loss at step 86 is 103.622908505\n",
      "Loss at step 87 is 103.712163533\n",
      "Loss at step 88 is 84.9877899519\n",
      "Loss at step 89 is 82.6245642887\n",
      "Loss at step 90 is 99.8338941279\n",
      "Loss at step 91 is 74.1203143359\n",
      "Loss at step 92 is 90.7583217132\n",
      "Loss at step 93 is 125.766023976\n",
      "Loss at step 94 is 97.9458453799\n",
      "Loss at step 95 is 105.596961557\n",
      "Loss at step 96 is 78.3043817012\n",
      "Loss at step 97 is 89.7344267924\n",
      "Loss at step 98 is 97.5218222536\n",
      "Loss at step 99 is 100.854710533\n",
      "Loss at step 100 is 74.5097536721\n",
      "Loss at step 101 is 121.855829304\n",
      "Loss at step 102 is 88.0494462713\n",
      "Loss at step 103 is 99.1658604015\n",
      "Loss at step 104 is 77.3423710765\n",
      "Loss at step 105 is 71.1790544666\n",
      "Loss at step 106 is 91.288113736\n",
      "Loss at step 107 is 85.5849494553\n",
      "Loss at step 108 is 75.2006775026\n",
      "Loss at step 109 is 101.836481339\n",
      "Loss at step 110 is 99.7122425368\n",
      "Loss at step 111 is 84.1700558555\n",
      "Loss at step 112 is 84.50471178\n",
      "Loss at step 113 is 91.6080558939\n",
      "Loss at step 114 is 71.91566693\n",
      "Loss at step 115 is 82.5961558919\n",
      "Loss at step 116 is 90.0884872499\n",
      "Loss at step 117 is 83.2891558646\n",
      "Loss at step 118 is 73.6577410478\n",
      "Loss at step 119 is 57.0076954273\n",
      "Loss at step 120 is 74.5132289103\n",
      "Loss at step 121 is 70.6071652931\n",
      "Loss at step 122 is 99.4452330403\n",
      "Loss at step 123 is 76.0105437061\n",
      "Loss at step 124 is 83.2836059285\n",
      "Loss at step 125 is 68.7548395501\n",
      "Loss at step 126 is 85.3737175442\n",
      "Loss at step 127 is 91.3182498928\n",
      "Loss at step 128 is 96.4827020234\n",
      "Loss at step 129 is 58.8625010847\n",
      "Loss at step 130 is 97.4538410177\n",
      "Loss at step 131 is 87.4895391567\n",
      "Loss at step 132 is 74.7980585007\n",
      "Loss at step 133 is 86.9633204046\n",
      "Loss at step 134 is 76.9577431235\n",
      "Loss at step 135 is 59.1490860082\n",
      "Loss at step 136 is 91.6896813575\n",
      "Loss at step 137 is 81.40907989\n",
      "Loss at step 138 is 74.1086782587\n",
      "Loss at step 139 is 89.6688391756\n",
      "Loss at step 140 is 83.6071709424\n",
      "Loss at step 141 is 67.3117225141\n",
      "Loss at step 142 is 69.0532391708\n",
      "Loss at step 143 is 56.6085467915\n",
      "Loss at step 144 is 71.551835086\n",
      "Loss at step 145 is 65.5898787974\n",
      "Loss at step 146 is 58.6775648711\n",
      "Loss at step 147 is 56.8975291342\n",
      "Loss at step 148 is 79.4240395682\n",
      "Loss at step 149 is 83.4421270053\n",
      "Loss at step 150 is 79.3590581992\n",
      "Loss at step 151 is 70.1434851443\n",
      "Loss at step 152 is 70.1007879206\n",
      "Loss at step 153 is 72.6223288539\n",
      "Loss at step 154 is 80.8619259816\n",
      "Loss at step 155 is 74.5976505279\n",
      "Loss at step 156 is 71.4497819199\n",
      "Loss at step 157 is 84.1237349181\n",
      "Loss at step 158 is 64.6104288219\n",
      "Loss at step 159 is 83.0345552564\n",
      "Loss at step 160 is 57.9690158349\n",
      "Loss at step 161 is 97.0424201075\n",
      "Loss at step 162 is 67.7929707751\n",
      "Loss at step 163 is 82.2597916395\n",
      "Loss at step 164 is 76.4778649458\n",
      "Loss at step 165 is 83.4826255337\n",
      "Loss at step 166 is 70.7282489369\n",
      "Loss at step 167 is 61.7667140092\n",
      "Loss at step 168 is 82.834764734\n",
      "Loss at step 169 is 75.4329001832\n",
      "Loss at step 170 is 65.2616595731\n",
      "Loss at step 171 is 54.5807332355\n",
      "Loss at step 172 is 80.6551077955\n",
      "Loss at step 173 is 67.2150456358\n",
      "Loss at step 174 is 63.4710756344\n",
      "Loss at step 175 is 78.3033428953\n",
      "Loss at step 176 is 56.2031136709\n",
      "Loss at step 177 is 64.9096091177\n",
      "Loss at step 178 is 59.7299414576\n",
      "Loss at step 179 is 48.2948632713\n",
      "Loss at step 180 is 80.0991116393\n",
      "Loss at step 181 is 67.0526248736\n",
      "Loss at step 182 is 71.5535876246\n",
      "Loss at step 183 is 60.8522604889\n",
      "Loss at step 184 is 71.2793406554\n",
      "Loss at step 185 is 57.334558836\n",
      "Loss at step 186 is 88.316125132\n",
      "Loss at step 187 is 72.0322878704\n",
      "Loss at step 188 is 73.7687944234\n",
      "Loss at step 189 is 48.5270837973\n",
      "Loss at step 190 is 62.27730466\n",
      "Loss at step 191 is 76.7921166177\n",
      "Loss at step 192 is 67.1590522481\n",
      "Loss at step 193 is 86.1342711589\n",
      "Loss at step 194 is 58.5224657365\n",
      "Loss at step 195 is 62.3749083896\n",
      "Loss at step 196 is 68.6517006689\n",
      "Loss at step 197 is 58.6333458209\n",
      "Loss at step 198 is 59.3995103143\n",
      "Loss at step 199 is 58.809148138\n",
      "Loss at step 200 is 54.5629963726\n",
      "Loss at step 201 is 60.3782333584\n",
      "Loss at step 202 is 60.612266373\n",
      "Loss at step 203 is 70.6628758183\n",
      "Loss at step 204 is 101.895988471\n",
      "Loss at step 205 is 74.5659204445\n",
      "Loss at step 206 is 72.0083253925\n",
      "Loss at step 207 is 66.4908343985\n",
      "Loss at step 208 is 64.2680291582\n",
      "Loss at step 209 is 75.0579098085\n",
      "Loss at step 210 is 54.746149116\n",
      "Loss at step 211 is 80.708273507\n",
      "Loss at step 212 is 61.3662866548\n",
      "Loss at step 213 is 62.2505183168\n",
      "Loss at step 214 is 51.7480552528\n",
      "Loss at step 215 is 59.1704391014\n",
      "Loss at step 216 is 58.8498719822\n",
      "Loss at step 217 is 52.3755936903\n",
      "Loss at step 218 is 63.7029463049\n",
      "Loss at step 219 is 79.5539341503\n",
      "Loss at step 220 is 65.6734148552\n",
      "Loss at step 221 is 68.3941053768\n",
      "Loss at step 222 is 58.626200559\n",
      "Loss at step 223 is 61.0602766043\n",
      "Loss at step 224 is 66.2997289232\n",
      "Loss at step 225 is 51.8779670674\n",
      "Loss at step 226 is 49.9075133887\n",
      "Loss at step 227 is 75.6248589121\n",
      "Loss at step 228 is 72.9483453016\n",
      "Loss at step 229 is 58.741194368\n",
      "Loss at step 230 is 58.1502007414\n",
      "Loss at step 231 is 51.2836472049\n",
      "Loss at step 232 is 55.8767021351\n",
      "Loss at step 233 is 56.8599112858\n",
      "Loss at step 234 is 63.3004638957\n",
      "Loss at step 235 is 73.4364610782\n",
      "Loss at step 236 is 67.3287344945\n",
      "Loss at step 237 is 62.4150875729\n",
      "Loss at step 238 is 60.8785291604\n",
      "Loss at step 239 is 69.6405859397\n",
      "Loss at step 240 is 52.5279527335\n",
      "Loss at step 241 is 56.846997317\n",
      "Loss at step 242 is 53.4268348132\n",
      "Loss at step 243 is 68.8144001947\n",
      "Loss at step 244 is 57.9178512974\n",
      "Loss at step 245 is 62.8459424649\n",
      "Loss at step 246 is 65.9545428068\n",
      "Loss at step 247 is 59.2600968162\n",
      "Loss at step 248 is 51.9825472946\n",
      "Loss at step 249 is 70.2482366748\n",
      "Loss at step 250 is 69.3079046606\n",
      "Loss at step 251 is 51.0310793881\n",
      "Loss at step 252 is 51.0664459086\n",
      "Loss at step 253 is 53.8113818161\n",
      "Loss at step 254 is 71.860297448\n",
      "Loss at step 255 is 51.8892841549\n",
      "Loss at step 256 is 51.0813930864\n",
      "Loss at step 257 is 55.1535388213\n",
      "Loss at step 258 is 55.8246041422\n",
      "Loss at step 259 is 59.3957909671\n",
      "Loss at step 260 is 65.9466376344\n",
      "Loss at step 261 is 38.2219658208\n",
      "Loss at step 262 is 58.7214667909\n",
      "Loss at step 263 is 59.1128641154\n",
      "Loss at step 264 is 84.5667281755\n",
      "Loss at step 265 is 63.5953133324\n",
      "Loss at step 266 is 65.6269254294\n",
      "Loss at step 267 is 61.1376389097\n",
      "Loss at step 268 is 66.7213621509\n",
      "Loss at step 269 is 55.665816845\n",
      "Loss at step 270 is 59.6090812943\n",
      "Loss at step 271 is 56.8545944751\n",
      "Loss at step 272 is 68.5308589063\n",
      "Loss at step 273 is 71.1346016042\n",
      "Loss at step 274 is 60.6462778828\n",
      "Loss at step 275 is 84.7993146403\n",
      "Loss at step 276 is 51.724736993\n",
      "Loss at step 277 is 55.3527737451\n",
      "Loss at step 278 is 46.6437509944\n",
      "Loss at step 279 is 57.9786259618\n",
      "Loss at step 280 is 52.8633861032\n",
      "Loss at step 281 is 59.0588103784\n",
      "Loss at step 282 is 72.6353205618\n",
      "Loss at step 283 is 70.6602008808\n",
      "Loss at step 284 is 49.128620938\n",
      "Loss at step 285 is 60.9393792384\n",
      "Loss at step 286 is 56.4742237095\n",
      "Loss at step 287 is 68.9734762334\n",
      "Loss at step 288 is 56.7222113297\n",
      "Loss at step 289 is 40.2404586843\n",
      "Loss at step 290 is 70.3281269275\n",
      "Loss at step 291 is 51.2759438266\n",
      "Loss at step 292 is 64.4837784043\n",
      "Loss at step 293 is 65.5578114667\n",
      "Loss at step 294 is 73.2922475036\n",
      "Loss at step 295 is 65.360032637\n",
      "Loss at step 296 is 54.6156335487\n",
      "Loss at step 297 is 66.534420912\n",
      "Loss at step 298 is 69.0029895335\n",
      "Loss at step 299 is 76.0599344143\n",
      "Loss at step 300 is 42.9869969949\n",
      "Loss at step 301 is 53.1154952379\n",
      "Loss at step 302 is 72.3524243052\n",
      "Loss at step 303 is 42.8017626986\n",
      "Loss at step 304 is 66.3636761833\n",
      "Loss at step 305 is 54.1266646697\n",
      "Loss at step 306 is 58.5983383864\n",
      "Loss at step 307 is 61.1197166953\n",
      "Loss at step 308 is 56.6803472119\n",
      "Loss at step 309 is 55.8178068677\n",
      "Loss at step 310 is 55.6910767188\n",
      "Loss at step 311 is 69.5149742203\n",
      "Loss at step 312 is 66.5181108538\n",
      "Loss at step 313 is 58.5656425857\n",
      "Loss at step 314 is 69.4994224158\n",
      "Loss at step 315 is 73.3692780389\n",
      "Loss at step 316 is 55.4007421382\n",
      "Loss at step 317 is 49.1806764444\n",
      "Loss at step 318 is 57.2761926958\n",
      "Loss at step 319 is 58.1446948274\n",
      "Loss at step 320 is 63.9141584154\n",
      "Loss at step 321 is 58.3386974506\n",
      "Loss at step 322 is 57.142990141\n",
      "Loss at step 323 is 47.5470942434\n",
      "Loss at step 324 is 65.1563424694\n",
      "Loss at step 325 is 44.2495356285\n",
      "Loss at step 326 is 45.1220574286\n",
      "Loss at step 327 is 59.5192058189\n",
      "Loss at step 328 is 64.7311595327\n",
      "Loss at step 329 is 63.3197779652\n",
      "Loss at step 330 is 59.3291100987\n",
      "Loss at step 331 is 53.7853689792\n",
      "Loss at step 332 is 84.1518048095\n",
      "Loss at step 333 is 58.1643506995\n",
      "Loss at step 334 is 48.0277454477\n",
      "Loss at step 335 is 44.1050000557\n",
      "Loss at step 336 is 48.6361622545\n",
      "Loss at step 337 is 64.2144171237\n",
      "Loss at step 338 is 49.6882305145\n",
      "Loss at step 339 is 84.321588667\n",
      "Loss at step 340 is 55.8628312028\n",
      "Loss at step 341 is 62.5353090906\n",
      "Loss at step 342 is 51.8041086925\n",
      "Loss at step 343 is 78.8550251392\n",
      "Loss at step 344 is 45.4202252855\n",
      "Loss at step 345 is 41.7740065195\n",
      "Loss at step 346 is 38.3205367841\n",
      "Loss at step 347 is 51.1112756583\n",
      "Loss at step 348 is 60.4505652933\n",
      "Loss at step 349 is 59.0583546277\n",
      "Loss at step 350 is 61.7377030827\n",
      "Loss at step 351 is 54.721180197\n",
      "Loss at step 352 is 72.977339551\n",
      "Loss at step 353 is 43.7956640049\n",
      "Loss at step 354 is 79.2838081651\n",
      "Loss at step 355 is 38.9002340295\n",
      "Loss at step 356 is 60.3217259717\n",
      "Loss at step 357 is 66.594223922\n",
      "Loss at step 358 is 46.9385940585\n",
      "Loss at step 359 is 55.2547836648\n",
      "Loss at step 360 is 57.8021555526\n",
      "Loss at step 361 is 50.227693289\n",
      "Loss at step 362 is 54.0033111651\n",
      "Loss at step 363 is 51.2617027084\n",
      "Loss at step 364 is 37.5817570151\n",
      "Loss at step 365 is 66.7382476917\n",
      "Loss at step 366 is 62.3836068014\n",
      "Loss at step 367 is 57.8936541086\n",
      "Loss at step 368 is 61.9854651469\n",
      "Loss at step 369 is 60.2554690747\n",
      "Loss at step 370 is 58.1318091817\n",
      "Loss at step 371 is 54.8669637834\n",
      "Loss at step 372 is 49.5427140448\n",
      "Loss at step 373 is 53.9833029418\n",
      "Loss at step 374 is 63.5800675052\n",
      "Loss at step 375 is 50.0147431879\n",
      "Loss at step 376 is 66.4012606746\n",
      "Loss at step 377 is 49.2043827025\n",
      "Loss at step 378 is 50.353322408\n",
      "Loss at step 379 is 42.0932021258\n",
      "Loss at step 380 is 49.2377531743\n",
      "Loss at step 381 is 63.5811865944\n",
      "Loss at step 382 is 46.0787345974\n",
      "Loss at step 383 is 36.4842607529\n",
      "Loss at step 384 is 62.4671730565\n",
      "Loss at step 385 is 51.8441985448\n",
      "Loss at step 386 is 49.1491432125\n",
      "Loss at step 387 is 61.2018923533\n",
      "Loss at step 388 is 46.5529659056\n",
      "Loss at step 389 is 55.4275715044\n",
      "Loss at step 390 is 60.3547126904\n",
      "Loss at step 391 is 48.2959602832\n",
      "Loss at step 392 is 62.2927768961\n",
      "Loss at step 393 is 59.3857862696\n",
      "Loss at step 394 is 77.4830454366\n",
      "Loss at step 395 is 53.1037244197\n",
      "Loss at step 396 is 47.6455930017\n",
      "Loss at step 397 is 60.6929064845\n",
      "Loss at step 398 is 59.7554901478\n",
      "Loss at step 399 is 38.8301445177\n",
      "The accuracy of your model is 11.5234375%!\n"
     ]
    }
   ],
   "source": [
    "new_weights, new_biases = least_square_optimization(training_data, training_response, weights, biases, batch_size, iterations, learning_rate)\n",
    "accuracy, index = evaluate(training_data, training_response, new_weights, new_biases, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(training_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for .05 step size, linear learning rate, loss of 88 at step 100, loss of 61 at step 200, 53 at step 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
