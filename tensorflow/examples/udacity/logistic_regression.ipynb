{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized logistic regression program in Python with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import os\n",
    "\n",
    "def import_from_csv(path, pixel_depth, length, class_num):\n",
    "    train_database = np.genfromtxt('{}'.format(path), delimiter=\",\", dtype=int)[1:,:]\n",
    "    train_labels = train_database[:,0].reshape(length,)\n",
    "    training_labels = np.eye(class_num)[train_labels]\n",
    "    training_data = np.delete(train_database,0,1)\n",
    "    return normalize(training_data, pixel_depth), training_labels, train_labels\n",
    "\n",
    "def normalize(image_data, pixel_depth):\n",
    "    data = (image_data - pixel_depth / 2) / pixel_depth\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#training_data = np.random.rand(20000, 200)\n",
    "#correct_index = np.random.choice(10, np.shape(training_data)[0])\n",
    "#training_labels = np.eye(10)[correct_index]\n",
    "\n",
    "#training_data = np.random.rand(10000, 784)\n",
    "#training_weights = np.random.rand(10, 784)\n",
    "#X = np.dot(training_data, training_weights.T)\n",
    "#training_labels = X / np.sum(X, axis=1).reshape(10000,1)\n",
    "\n",
    "training_data, training_labels, correct_index = import_from_csv('/Users/JAustin/Desktop/MNIST/train.csv', 255, 42000, 10)\n",
    "\n",
    "data_size = np.shape(training_data)[0] # 20000\n",
    "num_params = np.shape(training_data)[1] # 784\n",
    "num_classes = np.shape(training_labels)[1] # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1)[:,None]\n",
    "\n",
    "def predict(data, batch_size, weights, biases):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    output = np.dot(data[index], weights.T) + biases.T #np.dot(weights[:,np.newaxis], np.transpose(data[index])) + biases\n",
    "    return index, softmax(output)\n",
    "\n",
    "def evaluate(data, labels, weights, biases, batch_size):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    accuracy = (np.argmax(softmax(np.dot(data[index], weights.T) + biases.T), axis=1)==np.argmax(labels[index], axis=1)).sum() * 100 / batch_size\n",
    "    print(\"The accuracy of your model is %s%%!\" % accuracy)\n",
    "    return accuracy, index\n",
    "\n",
    "def gradient_descent(data, labels, weights, biases, batch_size, iterations, learning_rate, momentum_rate):\n",
    "    weight_momentum = np.zeros_like(weights)\n",
    "    bias_momentum = np.zeros_like(biases)\n",
    "    gamma = momentum_rate\n",
    "    for i in range(iterations):\n",
    "        index, prediction = predict(data, batch_size, weights, biases)\n",
    "        loss = - np.tensordot(labels[index], np.log(prediction), axes=2) / batch_size # + .1*np.linalg.norm(weights)**2 # cross entropy loss\n",
    "        if i % 50 == 0: print(\"Loss at step %s is %s\" % (i, loss))\n",
    "        error_arr = prediction - labels[index]\n",
    "        dW = np.sum(error_arr[..., None] * data[index][:, None, :], axis=0) / batch_size\n",
    "        dB = np.sum(error_arr, axis=0)[:, None] / batch_size\n",
    "        weight_momentum = gamma*weight_momentum + learning_rate[i]*dW\n",
    "        bias_momentum = gamma*bias_momentum + learning_rate[i]*dB\n",
    "        weights -= weight_momentum\n",
    "        biases -= bias_momentum\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.random.rand(num_classes, num_params) # or randn\n",
    "biases = np.random.rand(num_classes, 1)\n",
    "\n",
    "batch_size = 64\n",
    "iterations = 4000\n",
    "momentum_rate = .9\n",
    "\n",
    "learning_rate = np.array([.5*(1- x/iterations) for x in range(iterations)]) # linear learning rate\n",
    "# learning_rate = 1*np.exp(-5*np.arange(0,iterations)/iterations) # exponential learning rate # best results with .2\n",
    "# learning_rate = .01 * np.ones(iterations) # constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 8.06005566862\n",
      "Loss at step 50 is 5.92359180838\n",
      "Loss at step 100 is 5.38969714084\n",
      "Loss at step 150 is 1.22678561019\n",
      "Loss at step 200 is 2.43627242839\n",
      "Loss at step 250 is 1.44387168511\n",
      "Loss at step 300 is 2.10201195136\n",
      "Loss at step 350 is 1.54929052547\n",
      "Loss at step 400 is 3.55840642009\n",
      "Loss at step 450 is 1.10801998021\n",
      "Loss at step 500 is 1.79164786756\n",
      "Loss at step 550 is 0.868835384908\n",
      "Loss at step 600 is 1.046410642\n",
      "Loss at step 650 is 0.911849297774\n",
      "Loss at step 700 is 1.25705907134\n",
      "Loss at step 750 is 1.29870191831\n",
      "Loss at step 800 is 1.32913365288\n",
      "Loss at step 850 is 2.15079963299\n",
      "Loss at step 900 is 0.906734303932\n",
      "Loss at step 950 is 1.66850354744\n",
      "Loss at step 1000 is 1.07047299414\n",
      "Loss at step 1050 is 1.95134229418\n",
      "Loss at step 1100 is 1.02094186882\n",
      "Loss at step 1150 is 0.271660882765\n",
      "Loss at step 1200 is 0.723631463258\n",
      "Loss at step 1250 is 1.40132588618\n",
      "Loss at step 1300 is 1.78147129683\n",
      "Loss at step 1350 is 0.820488329445\n",
      "Loss at step 1400 is 0.677340150599\n",
      "Loss at step 1450 is 0.730346342161\n",
      "Loss at step 1500 is 0.4457906345\n",
      "Loss at step 1550 is 1.26927588283\n",
      "Loss at step 1600 is 2.17948945701\n",
      "Loss at step 1650 is 0.975936706249\n",
      "Loss at step 1700 is 1.34956923221\n",
      "Loss at step 1750 is 1.31407003801\n",
      "Loss at step 1800 is 1.35103760905\n",
      "Loss at step 1850 is 0.767784255187\n",
      "Loss at step 1900 is 0.946355116961\n",
      "Loss at step 1950 is 0.991617204676\n",
      "Loss at step 2000 is 0.399898040208\n",
      "Loss at step 2050 is 0.703239408826\n",
      "Loss at step 2100 is 0.850801085387\n",
      "Loss at step 2150 is 1.23104232334\n",
      "Loss at step 2200 is 0.433620231568\n",
      "Loss at step 2250 is 0.365649227587\n",
      "Loss at step 2300 is 0.442686357344\n",
      "Loss at step 2350 is 1.21666514451\n",
      "Loss at step 2400 is 0.601153747553\n",
      "Loss at step 2450 is 0.229423626877\n",
      "Loss at step 2500 is 0.300564189129\n",
      "Loss at step 2550 is 0.618358696347\n",
      "Loss at step 2600 is 0.220692105082\n",
      "Loss at step 2650 is 0.181864520978\n",
      "Loss at step 2700 is 0.230501628137\n",
      "Loss at step 2750 is 0.43802431123\n",
      "Loss at step 2800 is 0.81020585794\n",
      "Loss at step 2850 is 0.377610342556\n",
      "Loss at step 2900 is 0.426309724976\n",
      "Loss at step 2950 is 0.115164688904\n",
      "Loss at step 3000 is 0.87884369195\n",
      "Loss at step 3050 is 0.131323969819\n",
      "Loss at step 3100 is 0.448501743872\n",
      "Loss at step 3150 is 0.477714006195\n",
      "Loss at step 3200 is 0.246158657189\n",
      "Loss at step 3250 is 0.0810867481192\n",
      "Loss at step 3300 is 0.403628975518\n",
      "Loss at step 3350 is 0.187773217786\n",
      "Loss at step 3400 is 0.418554612495\n",
      "Loss at step 3450 is 0.55651040826\n",
      "Loss at step 3500 is 0.505758696588\n",
      "Loss at step 3550 is 0.771545199528\n",
      "Loss at step 3600 is 0.815001694716\n",
      "Loss at step 3650 is 0.427538159394\n",
      "Loss at step 3700 is 0.171288832828\n",
      "Loss at step 3750 is 0.479955866305\n",
      "Loss at step 3800 is 0.221213386771\n",
      "Loss at step 3850 is 0.255445489854\n",
      "Loss at step 3900 is 0.597310551946\n",
      "Loss at step 3950 is 0.278863169792\n",
      "The accuracy of your model is 93.75%!\n"
     ]
    }
   ],
   "source": [
    "new_weights, new_biases = gradient_descent(training_data, training_labels, weights, biases, batch_size, iterations, learning_rate, momentum_rate)\n",
    "accuracy, index = evaluate(training_data, training_labels, new_weights, new_biases, batch_size) # probably should be larger than batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = np.eye(10)[np.genfromtxt('{}'.format('/Users/JAustin/Desktop/MNIST/results_net4.csv'), delimiter=\",\", dtype=int)[1:,1]]\n",
    "test_data = normalize(np.genfromtxt('{}'.format('/Users/JAustin/Desktop/MNIST/test.csv'), delimiter=\",\", dtype=int)[1:,:], 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model is 92.2285714286%!\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_index = evaluate(test_data, test_labels, new_weights, new_biases, len(test_data)) # probably should be larger than batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution_arr = np.zeros((len(test_data)+1,2))\n",
    "solution_arr[1:,0] = np.linspace(1,len(test_data),len(test_data))\n",
    "solution_arr[1:,1] = np.argmax(softmax(np.dot(test_data, new_weights.T) + new_biases.T), axis=1)\n",
    "np.savetxt(\"results_self.csv\", solution_arr, fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Same logistic model with Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import os\n",
    "\n",
    "def import_from_csv(path, pixel_depth, length, class_num):\n",
    "    train_database = np.genfromtxt('{}'.format(path), delimiter=\",\", dtype=int)[1:,:]\n",
    "    train_labels = train_database[:,0].reshape(length,)\n",
    "    training_labels = np.eye(class_num)[train_labels]\n",
    "    training_data = np.delete(train_database,0,1)\n",
    "    return normalize(training_data, pixel_depth), training_labels, train_labels\n",
    "\n",
    "def normalize(image_data, pixel_depth):\n",
    "    data = (image_data - pixel_depth / 2) / pixel_depth\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#training_data = np.random.rand(20000, 200)\n",
    "#correct_index = np.random.choice(10, np.shape(training_data)[0])\n",
    "#training_labels = np.eye(10)[correct_index]\n",
    "\n",
    "#training_data = np.random.rand(10000, 784)\n",
    "#training_weights = np.random.rand(10, 784)\n",
    "#X = np.dot(training_data, training_weights.T)\n",
    "#training_labels = X / np.sum(X, axis=1).reshape(10000,1)\n",
    "\n",
    "training_data, training_labels, correct_index = import_from_csv('/Users/JAustin/Desktop/MNIST/train.csv', 255, 42000, 10)\n",
    "\n",
    "data_size = np.shape(training_data)[0] # 20000\n",
    "num_params = np.shape(training_data)[1] # 784\n",
    "num_classes = np.shape(training_labels)[1] # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1)[:,None]\n",
    "\n",
    "def predict(data, batch_size, weights, biases):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    output = np.dot(data[index], weights.T) + biases.T #np.dot(weights[:,np.newaxis], np.transpose(data[index])) + biases\n",
    "    return index, softmax(output)\n",
    "\n",
    "def evaluate(data, labels, weights, biases, batch_size):\n",
    "    index = np.random.choice(len(data), batch_size, replace=False)\n",
    "    accuracy = (np.argmax(softmax(np.dot(data[index], weights.T) + biases.T), axis=1)==np.argmax(labels[index], axis=1)).sum() * 100 / batch_size\n",
    "    print(\"The accuracy of your model is %s%%!\" % accuracy)\n",
    "    return accuracy, index\n",
    "\n",
    "def gradient_descent(data, labels, weights, biases, batch_size, iterations, learning_rate, momentum_rate):\n",
    "    weight_momentum = np.zeros_like(weights)\n",
    "    bias_momentum = np.zeros_like(biases)\n",
    "    gamma = momentum_rate\n",
    "    for i in range(iterations):\n",
    "        temp_weights = weights - gamma*weight_momentum\n",
    "        temp_biases = biases - gamma*bias_momentum\n",
    "        index, prediction = predict(data, batch_size, temp_weights, temp_biases)\n",
    "        loss = - np.tensordot(labels[index], np.log(prediction), axes=2) / batch_size # + .1*np.linalg.norm(weights)**2 # cross entropy loss\n",
    "        if i % 50 == 0: print(\"Loss at step %s is %s\" % (i, loss))\n",
    "        if i % 1000 ==0: evaluate(test_data, test_labels, weights, biases, len(test_data))\n",
    "        error_arr = prediction - labels[index]\n",
    "        dW = np.sum(error_arr[..., None] * data[index][:, None, :], axis=0) / batch_size\n",
    "        dB = np.sum(error_arr, axis=0)[:, None] / batch_size\n",
    "        weight_momentum = gamma*weight_momentum + learning_rate[i]*dW\n",
    "        bias_momentum = gamma*bias_momentum + learning_rate[i]*dB\n",
    "        weights -= weight_momentum\n",
    "        biases -= bias_momentum\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.random.rand(num_classes, num_params) # or randn\n",
    "biases = np.random.rand(num_classes, 1)\n",
    "\n",
    "batch_size = 64\n",
    "iterations = 8000\n",
    "momentum_rate = .9\n",
    "\n",
    "# learning_rate = np.array([1*(1- x/iterations) for x in range(iterations)]) # linear learning rate\n",
    "learning_rate = 1*np.exp(-5*np.arange(0,iterations)/iterations) # exponential learning rate # best results with .2\n",
    "# learning_rate = .01 * np.ones(iterations) # constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 5.25475619541\n",
      "The accuracy of your model is 7.57857142857%!\n",
      "Loss at step 50 is 7.27959367886\n",
      "Loss at step 100 is 3.0028517884\n",
      "Loss at step 150 is 2.59428100015\n",
      "Loss at step 200 is 2.96461203722\n",
      "Loss at step 250 is 2.60001133761\n",
      "Loss at step 300 is 1.03521271022\n",
      "Loss at step 350 is 2.46027328323\n",
      "Loss at step 400 is 1.3428843158\n",
      "Loss at step 450 is 1.61336694174\n",
      "Loss at step 500 is 2.43901149363\n",
      "Loss at step 550 is 1.29455865273\n",
      "Loss at step 600 is 1.51971525898\n",
      "Loss at step 650 is 0.509834151595\n",
      "Loss at step 700 is 2.33491309668\n",
      "Loss at step 750 is 2.04823223344\n",
      "Loss at step 800 is 2.33112023258\n",
      "Loss at step 850 is 0.364659446915\n",
      "Loss at step 900 is 1.17967382192\n",
      "Loss at step 950 is 1.07609615988\n",
      "Loss at step 1000 is 0.158047246613\n",
      "The accuracy of your model is 90.3642857143%!\n",
      "Loss at step 1050 is 0.171318693359\n",
      "Loss at step 1100 is 2.6864301851\n",
      "Loss at step 1150 is 0.886333719239\n",
      "Loss at step 1200 is 0.486163010456\n",
      "Loss at step 1250 is 0.895487085114\n",
      "Loss at step 1300 is 0.910093348474\n",
      "Loss at step 1350 is 0.757826876706\n",
      "Loss at step 1400 is 0.960563948522\n",
      "Loss at step 1450 is 0.604893183592\n",
      "Loss at step 1500 is 0.375096789217\n",
      "Loss at step 1550 is 0.524597346115\n",
      "Loss at step 1600 is 1.0527445737\n",
      "Loss at step 1650 is 0.733286983199\n",
      "Loss at step 1700 is 0.913758514912\n",
      "Loss at step 1750 is 0.964892177327\n",
      "Loss at step 1800 is 0.463421343989\n",
      "Loss at step 1850 is 2.01537527263\n",
      "Loss at step 1900 is 0.152879470118\n",
      "Loss at step 1950 is 0.927489559659\n",
      "Loss at step 2000 is 1.42215718951\n",
      "The accuracy of your model is 90.55%!\n",
      "Loss at step 2050 is 1.32436942867\n",
      "Loss at step 2100 is 0.102151843394\n",
      "Loss at step 2150 is 0.548879034043\n",
      "Loss at step 2200 is 1.00359154541\n",
      "Loss at step 2250 is 0.883538543943\n",
      "Loss at step 2300 is 1.1786713932\n",
      "Loss at step 2350 is 0.397177927383\n",
      "Loss at step 2400 is 0.84447949154\n",
      "Loss at step 2450 is 1.13077562235\n",
      "Loss at step 2500 is 1.41704572573\n",
      "Loss at step 2550 is 0.803738375297\n",
      "Loss at step 2600 is 0.940331754955\n",
      "Loss at step 2650 is 0.637338811805\n",
      "Loss at step 2700 is 0.588992417132\n",
      "Loss at step 2750 is 0.277969878133\n",
      "Loss at step 2800 is 0.824112550686\n",
      "Loss at step 2850 is 0.308236874892\n",
      "Loss at step 2900 is 0.507919345717\n",
      "Loss at step 2950 is 0.530667148151\n",
      "Loss at step 3000 is 0.822707406822\n",
      "The accuracy of your model is 90.0071428571%!\n",
      "Loss at step 3050 is 0.653964494691\n",
      "Loss at step 3100 is 0.832469049392\n",
      "Loss at step 3150 is 0.37691907437\n",
      "Loss at step 3200 is 0.237424219417\n",
      "Loss at step 3250 is 0.518173820002\n",
      "Loss at step 3300 is 0.104916403911\n",
      "Loss at step 3350 is 0.236659819866\n",
      "Loss at step 3400 is 0.466965876\n",
      "Loss at step 3450 is 0.159979073275\n",
      "Loss at step 3500 is 0.233992166447\n",
      "Loss at step 3550 is 0.148019317575\n",
      "Loss at step 3600 is 0.375224964936\n",
      "Loss at step 3650 is 0.377707261761\n",
      "Loss at step 3700 is 0.0771251964282\n",
      "Loss at step 3750 is 0.465287099837\n",
      "Loss at step 3800 is 0.156211288714\n",
      "Loss at step 3850 is 0.175002797146\n",
      "Loss at step 3900 is 0.970968985301\n",
      "Loss at step 3950 is 0.766926910553\n",
      "Loss at step 4000 is 0.264173291956\n",
      "The accuracy of your model is 91.4214285714%!\n",
      "Loss at step 4050 is 0.230591226228\n",
      "Loss at step 4100 is 0.190504095\n",
      "Loss at step 4150 is 0.766890726749\n",
      "Loss at step 4200 is 0.699312165593\n",
      "Loss at step 4250 is 0.33715650466\n",
      "Loss at step 4300 is 0.383680321951\n",
      "Loss at step 4350 is 0.283554475316\n",
      "Loss at step 4400 is 0.482127203069\n",
      "Loss at step 4450 is 0.439140741265\n",
      "Loss at step 4500 is 0.243685488545\n",
      "Loss at step 4550 is 0.101801099687\n",
      "Loss at step 4600 is 0.260627352555\n",
      "Loss at step 4650 is 0.13353050659\n",
      "Loss at step 4700 is 0.142782266647\n",
      "Loss at step 4750 is 0.838532293405\n",
      "Loss at step 4800 is 0.857205640941\n",
      "Loss at step 4850 is 0.969277681616\n",
      "Loss at step 4900 is 1.02630056715\n",
      "Loss at step 4950 is 0.381586597948\n",
      "Loss at step 5000 is 0.332534722586\n",
      "The accuracy of your model is 91.8357142857%!\n",
      "Loss at step 5050 is 0.136541121315\n",
      "Loss at step 5100 is 0.0905155072369\n",
      "Loss at step 5150 is 0.0692176885686\n",
      "Loss at step 5200 is 0.539777391548\n",
      "Loss at step 5250 is 0.902689544834\n",
      "Loss at step 5300 is 0.246717499298\n",
      "Loss at step 5350 is 0.334667483825\n",
      "Loss at step 5400 is 0.617665739656\n",
      "Loss at step 5450 is 0.17829874515\n",
      "Loss at step 5500 is 0.540504996329\n",
      "Loss at step 5550 is 0.434325454618\n",
      "Loss at step 5600 is 0.219224493129\n",
      "Loss at step 5650 is 0.550527240616\n",
      "Loss at step 5700 is 0.30000257699\n",
      "Loss at step 5750 is 0.846536289055\n",
      "Loss at step 5800 is 0.227003334369\n",
      "Loss at step 5850 is 0.215761416857\n",
      "Loss at step 5900 is 0.232094037456\n",
      "Loss at step 5950 is 0.200921461694\n",
      "Loss at step 6000 is 0.0743593584642\n",
      "The accuracy of your model is 91.9464285714%!\n",
      "Loss at step 6050 is 0.94851691576\n",
      "Loss at step 6100 is 0.169266968051\n",
      "Loss at step 6150 is 0.478330730061\n",
      "Loss at step 6200 is 0.337139942832\n",
      "Loss at step 6250 is 0.337764183568\n",
      "Loss at step 6300 is 0.350797702233\n",
      "Loss at step 6350 is 0.322283742932\n",
      "Loss at step 6400 is 0.074048765323\n",
      "Loss at step 6450 is 0.200960206176\n",
      "Loss at step 6500 is 0.53564152869\n",
      "Loss at step 6550 is 0.817052827256\n",
      "Loss at step 6600 is 0.121002442027\n",
      "Loss at step 6650 is 0.742673614611\n",
      "Loss at step 6700 is 0.351727915469\n",
      "Loss at step 6750 is 0.279152176609\n",
      "Loss at step 6800 is 0.181439510146\n",
      "Loss at step 6850 is 0.274057442443\n",
      "Loss at step 6900 is 0.202787889867\n",
      "Loss at step 6950 is 0.293991665094\n",
      "Loss at step 7000 is 0.481494602617\n",
      "The accuracy of your model is 91.9214285714%!\n",
      "Loss at step 7050 is 0.0431343264774\n",
      "Loss at step 7100 is 0.158338321742\n",
      "Loss at step 7150 is 0.235811530945\n",
      "Loss at step 7200 is 0.55269958422\n",
      "Loss at step 7250 is 0.1572542247\n",
      "Loss at step 7300 is 0.0263356578928\n",
      "Loss at step 7350 is 0.362903533926\n",
      "Loss at step 7400 is 0.215580763034\n",
      "Loss at step 7450 is 0.35317251921\n",
      "Loss at step 7500 is 0.168833002815\n",
      "Loss at step 7550 is 0.149053858479\n",
      "Loss at step 7600 is 0.184304813368\n",
      "Loss at step 7650 is 0.089965260438\n",
      "Loss at step 7700 is 0.0223837656284\n",
      "Loss at step 7750 is 0.30489270105\n",
      "Loss at step 7800 is 0.672870131544\n",
      "Loss at step 7850 is 0.516110069146\n",
      "Loss at step 7900 is 0.205183136281\n",
      "Loss at step 7950 is 0.150165456146\n",
      "The accuracy of your model is 93.75%!\n"
     ]
    }
   ],
   "source": [
    "new_weights, new_biases = gradient_descent(training_data, training_labels, weights, biases, batch_size, iterations, learning_rate, momentum_rate)\n",
    "accuracy, index = evaluate(training_data, training_labels, new_weights, new_biases, batch_size) # probably should be larger than batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = np.eye(10)[np.genfromtxt('{}'.format('/Users/JAustin/Desktop/MNIST/results_net4.csv'), delimiter=\",\", dtype=int)[1:,1]]\n",
    "test_data = normalize(np.genfromtxt('{}'.format('/Users/JAustin/Desktop/MNIST/test.csv'), delimiter=\",\", dtype=int)[1:,:], 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model is 91.9428571429%!\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_index = evaluate(test_data, test_labels, new_weights, new_biases, len(test_data)) # probably should be larger than batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution_arr = np.zeros((len(test_data)+1,2))\n",
    "solution_arr[1:,0] = np.linspace(1,len(test_data),len(test_data))\n",
    "solution_arr[1:,1] = np.argmax(softmax(np.dot(test_data, new_weights.T) + new_biases.T), axis=1)\n",
    "np.savetxt(\"results_self.csv\", solution_arr, fmt='%i', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
